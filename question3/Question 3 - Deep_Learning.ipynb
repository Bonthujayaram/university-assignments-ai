{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# [1] Setup: imports, mount Drive, config\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---- Hyperparameters (updated) ----\n",
        "EMBED_DIM      = 128     # d: character embedding size (was 64)\n",
        "HIDDEN_DIM     = 256     # h: hidden size for encoder & decoder (was 128)\n",
        "NUM_LAYERS     = 1       # number of RNN/LSTM/GRU layers\n",
        "CELL_TYPE      = \"gru\"   # \"rnn\", \"gru\", or \"lstm\"\n",
        "BATCH_SIZE     = 64\n",
        "N_EPOCHS       = 20      # was 10\n",
        "LEARNING_RATE  = 1e-3\n",
        "TEACHER_FORCE  = 0.5\n",
        "MAX_DATA_ROWS  = 6000    # use subset while debugging; set None to use all\n",
        "# ----------------------------------\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [2] Load Aksharantar Hindi dataset from Drive (no header)\n",
        "# ============================================================\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/aksharantar_sampled/hin/hin_train.csv\"\n",
        "assert os.path.exists(data_path), f\"File not found: {data_path}\"\n",
        "\n",
        "# File has NO header row, so we set header=None and give names\n",
        "df = pd.read_csv(data_path, header=None, names=[\"src\", \"tgt\"]).dropna()\n",
        "\n",
        "if MAX_DATA_ROWS is not None:\n",
        "    df = df.iloc[:MAX_DATA_ROWS].reset_index(drop=True)\n",
        "\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Column names for the rest of the code\n",
        "SRC_COL = \"src\"   # romanized (Latin) input\n",
        "TGT_COL = \"tgt\"   # Devanagari output\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [3] Build character vocabularies for source & target\n",
        "# ============================================================\n",
        "\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "def build_char_vocab(texts):\n",
        "    chars = set()\n",
        "    for t in texts:\n",
        "        for ch in str(t):\n",
        "            chars.add(ch)\n",
        "    char_list = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(list(chars))\n",
        "    stoi = {ch: i for i, ch in enumerate(char_list)}\n",
        "    itos = {i: ch for ch, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "src_stoi, src_itos = build_char_vocab(df[SRC_COL].tolist())\n",
        "tgt_stoi, tgt_itos = build_char_vocab(df[TGT_COL].tolist())\n",
        "\n",
        "SRC_VOCAB_SIZE = len(src_stoi)\n",
        "TGT_VOCAB_SIZE = len(tgt_stoi)\n",
        "\n",
        "print(\"Source vocab size:\", SRC_VOCAB_SIZE)\n",
        "print(\"Target vocab size:\", TGT_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [4] Encoding utilities & Dataset / DataLoader\n",
        "# ============================================================\n",
        "\n",
        "def encode_src(text):\n",
        "    \"\"\"Encode Latin/romanized string to list of source token IDs.\"\"\"\n",
        "    return [src_stoi.get(ch, src_stoi[UNK_TOKEN]) for ch in str(text)]\n",
        "\n",
        "def encode_tgt(text):\n",
        "    \"\"\"Encode Devanagari string with <sos> and <eos> tokens.\"\"\"\n",
        "    return (\n",
        "        [tgt_stoi[SOS_TOKEN]] +\n",
        "        [tgt_stoi.get(ch, tgt_stoi[UNK_TOKEN]) for ch in str(text)] +\n",
        "        [tgt_stoi[EOS_TOKEN]]\n",
        "    )\n",
        "\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.src_texts = df[SRC_COL].tolist()\n",
        "        self.tgt_texts = df[TGT_COL].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = encode_src(self.src_texts[idx])\n",
        "        tgt_ids = encode_tgt(self.tgt_texts[idx])\n",
        "        return torch.tensor(src_ids, dtype=torch.long), \\\n",
        "               torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Pads sequences in the batch to the max length.\n",
        "    Returns: padded_src, padded_tgt, src_lengths, tgt_lengths\n",
        "    \"\"\"\n",
        "    src_seqs, tgt_seqs = zip(*batch)\n",
        "    src_lengths = [len(s) for s in src_seqs]\n",
        "    tgt_lengths = [len(t) for t in tgt_seqs]\n",
        "\n",
        "    max_src = max(src_lengths)\n",
        "    max_tgt = max(tgt_lengths)\n",
        "\n",
        "    padded_src = []\n",
        "    padded_tgt = []\n",
        "\n",
        "    for s, t in zip(src_seqs, tgt_seqs):\n",
        "        src_pad_len = max_src - len(s)\n",
        "        tgt_pad_len = max_tgt - len(t)\n",
        "\n",
        "        padded_src.append(\n",
        "            torch.cat([s, torch.full((src_pad_len,), src_stoi[PAD_TOKEN], dtype=torch.long)])\n",
        "        )\n",
        "        padded_tgt.append(\n",
        "            torch.cat([t, torch.full((tgt_pad_len,), tgt_stoi[PAD_TOKEN], dtype=torch.long)])\n",
        "        )\n",
        "\n",
        "    padded_src = torch.stack(padded_src)  # [batch, max_src]\n",
        "    padded_tgt = torch.stack(padded_tgt)  # [batch, max_tgt]\n",
        "\n",
        "    return padded_src, padded_tgt, torch.tensor(src_lengths), torch.tensor(tgt_lengths)\n",
        "\n",
        "dataset = TransliterationDataset(df)\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "print(\"Number of training examples:\", len(dataset))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [5] RNN factory, Encoder, Decoder, Seq2Seq model\n",
        "# ============================================================\n",
        "\n",
        "def make_rnn(cell_type, input_size, hidden_size, num_layers=1, batch_first=True):\n",
        "    \"\"\"\n",
        "    Helper to create RNN/LSTM/GRU module based on `cell_type`.\n",
        "    \"\"\"\n",
        "    cell_type = cell_type.lower()\n",
        "    if cell_type == \"lstm\":\n",
        "        return nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                       batch_first=batch_first)\n",
        "    elif cell_type == \"gru\":\n",
        "        return nn.GRU(input_size, hidden_size, num_layers=num_layers,\n",
        "                      batch_first=batch_first)\n",
        "    elif cell_type == \"rnn\":\n",
        "        return nn.RNN(input_size, hidden_size, num_layers=num_layers,\n",
        "                      batch_first=batch_first, nonlinearity=\"tanh\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown CELL_TYPE: {cell_type}\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level encoder:\n",
        "    - Embedding layer\n",
        "    - RNN/LSTM/GRU\n",
        "    Returns final hidden state(s).\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, cell_type=\"gru\"):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.lower()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=src_stoi[PAD_TOKEN])\n",
        "        self.rnn = make_rnn(self.cell_type, embed_dim, hidden_dim, num_layers)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: [batch, src_len]\n",
        "        embedded = self.embedding(src)  # [batch, src_len, embed_dim]\n",
        "\n",
        "        # Pack for variable-length RNN\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        outputs, hidden = self.rnn(packed)\n",
        "        # outputs: packed sequence (ignored)\n",
        "        # hidden: h_n (and c_n if LSTM)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-level decoder:\n",
        "    - Embedding layer\n",
        "    - RNN/LSTM/GRU\n",
        "    - Linear layer to vocab logits\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, cell_type=\"gru\"):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.cell_type = cell_type.lower()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=tgt_stoi[PAD_TOKEN])\n",
        "        self.rnn = make_rnn(self.cell_type, embed_dim, hidden_dim, num_layers)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_step, hidden):\n",
        "        # input_step: [batch] (token ids for one time step)\n",
        "        input_step = input_step.unsqueeze(1)        # [batch, 1]\n",
        "        embedded = self.embedding(input_step)       # [batch, 1, embed_dim]\n",
        "        output, hidden = self.rnn(embedded, hidden) # output: [batch, 1, hidden_dim]\n",
        "        logits = self.fc_out(output.squeeze(1))     # [batch, vocab_size]\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Full Seq2Seq model:\n",
        "    - Encoder processes entire input sequence\n",
        "    - Decoder generates output sequence step-by-step\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, cell_type=\"gru\"):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type = cell_type.lower()\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing=0.5):\n",
        "        \"\"\"\n",
        "        src: [batch, src_len]\n",
        "        tgt: [batch, tgt_len] with <sos> at index 0\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len = tgt.size()\n",
        "        vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=src.device)\n",
        "\n",
        "        # Encode\n",
        "        hidden = self.encoder(src, src_lengths)\n",
        "\n",
        "        # First decoder input is <sos> for everyone\n",
        "        input_token = tgt[:, 0]  # [batch]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            logits, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t] = logits\n",
        "\n",
        "            teacher = random.random() < teacher_forcing\n",
        "            top1 = logits.argmax(1)  # [batch]\n",
        "\n",
        "            input_token = tgt[:, t] if teacher else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "encoder = Encoder(SRC_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM,\n",
        "                  num_layers=NUM_LAYERS, cell_type=CELL_TYPE)\n",
        "decoder = Decoder(TGT_VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM,\n",
        "                  num_layers=NUM_LAYERS, cell_type=CELL_TYPE)\n",
        "model = Seq2Seq(encoder, decoder, cell_type=CELL_TYPE).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[PAD_TOKEN])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [6] Training loop (updated epochs)\n",
        "# ============================================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, teacher_forcing=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for src, tgt, src_lens, tgt_lens in loader:\n",
        "        src, tgt, src_lens = src.to(device), tgt.to(device), src_lens.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(src, src_lens, tgt, teacher_forcing=teacher_forcing)\n",
        "        # outputs: [batch, tgt_len, vocab_size]\n",
        "\n",
        "        # Ignore <sos> position for loss\n",
        "        logits = outputs[:, 1:].reshape(-1, outputs.size(-1))   # [batch*(tgt_len-1), vocab]\n",
        "        targets = tgt[:, 1:].reshape(-1)                        # [batch*(tgt_len-1)]\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_one_epoch(model, train_loader, optimizer, criterion,\n",
        "                           teacher_forcing=TEACHER_FORCE)\n",
        "    print(f\"Epoch {epoch:02d}/{N_EPOCHS} - Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [7] Inference: transliterate new words\n",
        "# ============================================================\n",
        "\n",
        "def decode_tokens(token_ids):\n",
        "    chars = []\n",
        "    for idx in token_ids:\n",
        "        idx = int(idx)\n",
        "        ch = tgt_itos.get(idx, UNK_TOKEN)\n",
        "        if ch == EOS_TOKEN:\n",
        "            break\n",
        "        if ch not in [PAD_TOKEN, SOS_TOKEN]:\n",
        "            chars.append(ch)\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def transliterate(model, word, max_len=30):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = torch.tensor([encode_src(word)], dtype=torch.long).to(device)\n",
        "        src_len = torch.tensor([src_ids.size(1)], dtype=torch.long).to(device)\n",
        "\n",
        "        hidden = model.encoder(src_ids, src_len)\n",
        "        input_token = torch.tensor([tgt_stoi[SOS_TOKEN]], dtype=torch.long).to(device)\n",
        "\n",
        "        outputs = []\n",
        "        for _ in range(max_len):\n",
        "            logits, hidden = model.decoder(input_token, hidden)\n",
        "            top1 = logits.argmax(1)\n",
        "            outputs.append(top1.item())\n",
        "            input_token = top1\n",
        "            if top1.item() == tgt_stoi[EOS_TOKEN]:\n",
        "                break\n",
        "\n",
        "    return decode_tokens(outputs)\n",
        "\n",
        "# Test some random examples from training data\n",
        "print(\"\\nSample transliterations:\")\n",
        "for _ in range(10):\n",
        "    src_word = random.choice(df[SRC_COL].tolist())\n",
        "    print(src_word, \"->\", transliterate(model, src_word))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# [8] Simple character-level accuracy evaluation\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_char_accuracy(model, df, num_samples=500):\n",
        "    \"\"\"\n",
        "    Simple character-level accuracy over first num_samples rows.\n",
        "    Compares predicted vs true target character-by-character.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_chars = 0\n",
        "    correct_chars = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(num_samples, len(df))):\n",
        "            src = str(df.iloc[i][SRC_COL])\n",
        "            tgt = str(df.iloc[i][TGT_COL])\n",
        "            pred = transliterate(model, src)\n",
        "\n",
        "            # Compare up to min length\n",
        "            L = min(len(tgt), len(pred))\n",
        "            for j in range(L):\n",
        "                total_chars += 1\n",
        "                if tgt[j] == pred[j]:\n",
        "                    correct_chars += 1\n",
        "\n",
        "    if total_chars == 0:\n",
        "        return 0.0\n",
        "\n",
        "    acc = correct_chars / total_chars\n",
        "    return acc\n",
        "\n",
        "char_acc = evaluate_char_accuracy(model, df, num_samples=500)\n",
        "print(f\"\\nApprox. character-level accuracy on first 500 samples: {char_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N6LlPBvLFq0",
        "outputId": "cec939f6-675d-4253-9436-aa6f45561100"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "Data shape: (6000, 2)\n",
            "           src         tgt\n",
            "0  shastragaar  शस्त्रागार\n",
            "1      bindhya    बिन्द्या\n",
            "2    kirankant    किरणकांत\n",
            "3  yagyopaveet   यज्ञोपवीत\n",
            "4      ratania     रटानिया\n",
            "Source vocab size: 30\n",
            "Target vocab size: 66\n",
            "Number of training examples: 6000\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(30, 128, padding_idx=0)\n",
            "    (rnn): GRU(128, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(66, 128, padding_idx=0)\n",
            "    (rnn): GRU(128, 256, batch_first=True)\n",
            "    (fc_out): Linear(in_features=256, out_features=66, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 01/20 - Loss: 3.0970\n",
            "Epoch 02/20 - Loss: 2.3244\n",
            "Epoch 03/20 - Loss: 1.9072\n",
            "Epoch 04/20 - Loss: 1.6440\n",
            "Epoch 05/20 - Loss: 1.4464\n",
            "Epoch 06/20 - Loss: 1.3225\n",
            "Epoch 07/20 - Loss: 1.1712\n",
            "Epoch 08/20 - Loss: 1.0786\n",
            "Epoch 09/20 - Loss: 1.0069\n",
            "Epoch 10/20 - Loss: 0.9285\n",
            "Epoch 11/20 - Loss: 0.8462\n",
            "Epoch 12/20 - Loss: 0.8116\n",
            "Epoch 13/20 - Loss: 0.7266\n",
            "Epoch 14/20 - Loss: 0.6381\n",
            "Epoch 15/20 - Loss: 0.6019\n",
            "Epoch 16/20 - Loss: 0.5841\n",
            "Epoch 17/20 - Loss: 0.5075\n",
            "Epoch 18/20 - Loss: 0.4530\n",
            "Epoch 19/20 - Loss: 0.4081\n",
            "Epoch 20/20 - Loss: 0.3817\n",
            "\n",
            "Sample transliterations:\n",
            "santoshani -> संतोषनी\n",
            "rangari -> रंगारी\n",
            "sudharanyasathiche -> सुधारण्यासाठीचे\n",
            "sugauli -> सुगुली\n",
            "harmohinder -> हरमोहिन्दर\n",
            "fired -> फियर्ड\n",
            "styling -> स्टायलिंग\n",
            "karanje -> करंजे\n",
            "jansamanyansah -> जनसामानायांसह\n",
            "samaldas -> समलदास\n",
            "\n",
            "Approx. character-level accuracy on first 500 samples: 88.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlTBdgQjLyi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}